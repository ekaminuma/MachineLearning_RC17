{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "補助本「ゼロから作るDeep Learning」\n",
    "3章～7章までを輪読会の補足として、読んで行きます。\n",
    "Eli Kaminuma 2018.1.31\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 補助本5章 [2018.2.26]\n",
    "# 5章　誤差逆伝播法\n",
    "\n",
    "- 前4章で数値微分で重みパラメータ勾配を求めた→時間かかる\n",
    "- 5章で計算効率の高い「誤差逆伝播法」を、「数式」でなく「計算グラフ」で学ぶ。\n",
    "- 誤差逆伝播法を計算グラフで学ぶアイデアは下記を参考\n",
    " - Andrej Karpathyのブログ [4] \n",
    " - 彼と Fei-Fei Li 教授ら作：スタンフォード大学のディープラーニング授業「CS231n」[5]\n",
    " \n",
    " \n",
    " \n",
    "- 目次\n",
    "\n",
    "```\n",
    "5.1 計算グラフ \n",
    "5.1.1 計算グラフで解く \n",
    "5.1.2 局所的な計算 \n",
    "5.1.3 なぜ計算グラフで解くのか?\n",
    "\n",
    "5.2 連鎖律\n",
    "5.2.1 計算グラフの逆伝播 \n",
    "5.2.2 連鎖律とは \n",
    "5.2.3 連鎖律と計算グラフ \n",
    "\n",
    "5.3 逆伝播 \n",
    "5.3.1 加算ノードの逆伝播 \n",
    "5.3.2 乗算ノードの逆伝播 \n",
    "5.3.3 リンゴの例 \n",
    "\n",
    "5.4 単純なレイヤの実装 \n",
    "5.4.1 乗算レイヤの実装 \n",
    "5.4.2 加算レイヤの実装 \n",
    "\n",
    "5.5 活性化関数レイヤの実装 \n",
    "5.5.1 ReLU レイヤ \n",
    "5.5.2 Sigmoid レイヤ \n",
    "\n",
    "5.6 Affine / Softmax レイヤの実装\n",
    "5.6.1 Affine レイヤ\n",
    "5.6.2 バッチ版 Affine レイヤ \n",
    "5.6.3 Softmax-with-Loss レイヤ·\n",
    "\n",
    "5.7 誤差逆伝播法の実装 \n",
    "5.7.1 ニューラルネットワークの学習の全体図 \n",
    "5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装 \n",
    "5.7.3 誤差逆伝播法の勾配確認 \n",
    "5.7.4 誤差逆伝播法を使った学習 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#前準備\n",
    "import numpy as np\n",
    "import sys, os, time\n",
    "from collections import OrderedDict # 順序付きDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.pardir) # load_mnistするために親ディレクトリを追加。必要に応じて変更 or dataset.mnistを別途import\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## これまでの章で使った関数のインポート。 \n",
    "## 本来は下記の2つのインポート文でも良いが、参照しやすいように記載\n",
    "# from common.layers import *\n",
    "# from common.gradient import numerical_gradient\n",
    "\n",
    "# 3.2 章\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "# 4.4章より, 微分を使った勾配算出用 \n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# 3章より\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# 4章より\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 5.1 計算グラフ\n",
    "\n",
    "- 計算グラフとは、計算の過程をグラフによって表したもの\n",
    "- 順伝播（左→右）と逆伝播（右→左）\n",
    "- 逆伝播は、微分を効率よく計算\n",
    "- グラフとは、データ構造としてのグラフ(複数のノードとエッジで構成）を意味する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 5.1.1 計算グラフで解く\n",
    "\n",
    "```\n",
    "問 1:太郎くんはスーパーで 1 個 100 円のリンゴを 2 個買いました。支払う金額\n",
    "を求めなさい。ただし、消費税が 10% 適用されるものとします。\n",
    "```\n",
    "\n",
    "- 計算グラフはノードと矢印によって計算の過程を表す\n",
    "- ノードは○で表記し、○の中に演算の内容を書く。\n",
    "- 計算の途中結果を矢印の上部に書く(ノードごとの計算結果を左→右に表現)。\n",
    "- FIG5-1   apple -(100)-> (x2) -(200)-> (x1.1) -(220)-> paid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次の問題は、加算ノード「＋」が加わり、りんごとミカンの金額を合算する。\n",
    "```\n",
    "問 2:太郎くんはスーパーでリンゴを 2 個、みかんを 3 個買いました。リンゴは\n",
    "1 個 100 円、みかんは 1 個 150 円です。消費税が 10% かかるものとして、支払\n",
    "う金額を求めなさい。\n",
    "```\n",
    "\n",
    "計算グラフの問題を解くには、\n",
    "\n",
    "- 計算グラフを構築\n",
    "- 計算グラフ上で計算を左から右へと進める(順伝播:forward propagation)\n",
    "- 順伝播の逆を逆伝播(back propagation)と言う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2    局所的な計算\n",
    "\n",
    "- 計算グラフの特徴は、「局所的な計算」を伝播すること\n",
    "- 局所的とは、「自分に関係する小さな範囲」\n",
    "- 局所的な計算の結果を伝達することで、全体の複雑な計算の結果が得られる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3   なぜ計算グラフで解くのか?\n",
    "\n",
    "- 計算グラフの利点\n",
    " - 最大の利点は、逆方向の伝播によって「微分」を効率良く計算できる点\n",
    " - 局所的な計算によって、各ノードでは単純な計算に集中（問題を単純化）\n",
    " - 計算グラフによって、途中の計算の結果をすべて保持\n",
    "\n",
    "- 例：リンゴの値段が値上がりした場合、最終支払金額への影響は？\n",
    "- これは「リンゴの値段に関する支払金額の微分」を求めることに相当\n",
    "- リンゴの値段を x、支払金額を L とした場合、\n",
    "  ∂L/∂x を求めることに相当\n",
    "\n",
    "- 逆伝播は右から左へ「1 → 1.1 → 2.2」と微分の値が伝達\n",
    "- 「リンゴの値段に関する支払金額の微分」の値は 2.2 \n",
    "- リンゴが 1 円値上がりしたら、最終支払金額が 2.2 円増える\n",
    "- 途中まで求めた微分(途中まで流れた微分)の結果を共有\n",
    "- 計算グラフの利点は、順伝播と逆伝播で各変数の微分値を効率良く求める点\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.2 連鎖律\n",
    "\n",
    "- 「局所的な微分」を伝達する原理は、連鎖律(chain rule)による。\n",
    "- 連鎖律が計算グラフ上での逆伝播に対応することを明らかにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 計算グラフの逆伝播\n",
    "\n",
    "- 計算グラフを使った逆伝播の例\n",
    "- y = f(x) という計算、この計算の逆伝播を図5-6に表す\n",
    "- 逆伝播の計算手順は、信号Eに対して、ノードの局所的な微分\n",
    "　(∂y/∂x)を乗算し、それを次のノードへ伝達していく。\n",
    "- 局所的な微分とは、順伝播での y = f(x) という計算の微分を求めること\n",
    "- xに関する y の微分(∂y∂x)を求めることを意味する。\n",
    "- たとえば、y = f(x) = x^2 だとしたら、∂y/∂x = 2x 。\n",
    "- 局所的な微分を上流から伝達された値(例では E)に乗算して、前ノードへと渡す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 連鎖律とは\n",
    "\n",
    "- 合成関数 = 複数の関数によって構成される関数\n",
    "- Z=(x+y)^2は、Z=t^2とt=(x+y)の合成関数\n",
    "\n",
    "- 連鎖律とは合成関数の微分についての性質であり、次のように定義される\n",
    "\n",
    "```\n",
    "ある関数が合成関数で表される場合、合成関数の微分は、\n",
    "合成関数を構成するそれぞれの関数の微分の積によって\n",
    "表すことができる。\n",
    "```\n",
    "\n",
    "- これを連鎖律の原理と言う。\n",
    "- 式 (5.1) の例で、∂z/∂xは、∂z/∂tと ∂t/∂xの積によって表す。\n",
    "\n",
    "```\n",
    "∂z/∂x = (∂z/∂t)*(∂t/∂x)\n",
    "\n",
    "(∂z/∂t)= 2t\n",
    "(∂t/∂x)=1\n",
    "\n",
    "∂z/∂x = (∂z/∂t)*(∂t/∂x)= 2t*1=2(x+y)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 連鎖律と計算グラフ\n",
    "\n",
    "- 図5-7 式 (5.4) の計算グラフ:順方向とは逆向きの方向に、局所的微分を乗算して渡す\n",
    "- 図5-7 で注目すべきは、一番左の逆伝播の結果。これは、連鎖律より、\n",
    "(∂z/∂z)(∂z/∂t)(∂t/∂x) =(∂z/∂t)(∂t/∂x) =(∂z/∂x) が\n",
    "成り立ち、「x に関する z の微分」に対応する。\n",
    "つまり、逆伝播が行っていることは、連鎖律の原理から構成される。\n",
    "- 図5-8 計算グラフの逆伝播の結果より、 ∂z/∂x は 2(x + y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 逆伝播\n",
    "\n",
    "- 前節では、計算グラフの逆伝播が連鎖律によって成り立つことを説明した\n",
    "- 「+」や「×」などの演算を例に、逆伝播の仕組みについて説明する。\n",
    "- 加算ノード： そのまま伝播. z = x + y の偏微分は共に1。\n",
    "- 乗算ノード： ひっくり返した値(xならy,yならx)を乗算し伝播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 加算ノードの逆伝播\n",
    "\n",
    "- 加算ノードの逆伝播は、z = x + y 対象に逆伝播を見る。\n",
    "\n",
    "- 図 5-9 加算ノードの逆伝播:左図が順伝播、右図が逆伝播。右図の逆伝播が示すように、加算ノードの逆伝播は、上流の値をそのまま下流へ流す\n",
    "\n",
    "- 図 5-10 最終的に出力する計算の一部に、今回の加算ノードが存在する。逆伝播の際には、一番右の出力からスタートして、局所的な微分がノードからノードへと逆方向に伝播されていく\n",
    "\n",
    "- 加算の逆伝播具体例\n",
    "-「10 + 5 = 15」という計算があるとして、逆伝播の際には、\n",
    " 上流から 1.3 の値が流れてくるとします。これを計算グラフで書くと\n",
    " 図5-11(加算ノードの逆伝播の具体例)\n",
    " - 加算ノードの逆伝播は入力信号を次のノードへ出力するだけなので、図5-11 のように、1.3 を次のノードへと流します。\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2  乗算ノードの逆伝播\n",
    "\n",
    "続いて、乗算ノードの逆伝播（z = xy を例に）\n",
    "\n",
    "- 乗算の逆伝播の場合は、上流の値に順伝播の入力信号を“ひっくり返した値”を乗算して下流へ流す。\n",
    "- ひっくり返した値(図5-12)\n",
    "　- 順伝播の際に x の信号であれば逆伝播では y\n",
    "  - 順伝播の際に y の信号であれば逆伝播では x \n",
    "- 具体例: 「10 × 5 = 50」\n",
    "  - 逆伝播の際に上流から 1.3 の値が流れてくる\n",
    "  - これを計算グラフで書くと図5-13 \n",
    "  - 乗算の逆伝播は1.3 × 5 = 6.5、1.3 × 10 = 13 \n",
    "  - なお加算の逆伝播では、上流の値をただ下流に流すだけ\n",
    "  - 順伝播の入力信号の値は必要なし\n",
    "  - 乗算の逆伝播は、順伝播のときの入力信号必要(信号保持)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 リンゴの例\n",
    "\n",
    "- リンゴ 2 個と消費税\n",
    "- リンゴの値段、リンゴの個数、消費税の3変数が最終支払金額にどう影響するか。\n",
    "- これは以下の3つを求める事に相当\n",
    "  -「リンゴの値段に関する支払金額の微分」\n",
    "  -「リンゴの個数に関する支払金額の微分」\n",
    "  - 「消費税に関する支払金額の微分」\n",
    "- これを計算グラフの逆伝播を使って解く（図5-14 ）\n",
    "- 図 5-14 リンゴの買い物の逆伝播の例\n",
    "  - 消費税とリンゴの値段が同じ量だけ増加したら、消費税は 200 の大きさで最終的な支払金額に影響を与え、リンゴの値段は 2.2 の大きさで影響を与える。\n",
    "- 消費税とリンゴの値段はスケールが異なる\n",
    "  - 消費税の1は100%\n",
    "  - リンゴの値段の1は1円\n",
    "- 図 5-15 リンゴとみかんの買い物の逆伝播の例\n",
    "  - 四角に数字を入れて逆伝播を完成させよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 単純なレイヤの実装\n",
    "\n",
    "実際にニューラルネットワークを構築するときに、各レイヤの組み合わせだけで簡単に計算できるようになる。\n",
    "\n",
    "- 「リンゴの買い物」の例を、Python で実装\n",
    "　　- 乗算ノードを「乗算レイヤ(MulLayer)」\n",
    "    - 加算ノードを「加算レイヤ(AddLayer)」\n",
    "-  次節ではニューラルネットワークの「層(レイヤ)」を1クラスで実装\n",
    "  - 「レイヤ」とは、ニューラルネットワークにおける機能単位\n",
    "  - 　下記はレイヤ単位で実装\n",
    "     - シグモイド関数のための Sigmoid \n",
    "     - 行列の積のための Affine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.4.1 乗算レイヤの実装\n",
    "\n",
    "- レイヤは共通methodを持つ\n",
    "   - forward() 順伝播 \n",
    "   - backward() 逆伝播\n",
    "- 乗算レイヤは MulLayer クラス\n",
    "- ソー スコードはch05/layer_naive.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "```\n",
    "\n",
    "- __init__() では、インスタンス変数 (x,y) を初期化\n",
    "- 順伝播時の入力値を保持。\n",
    "  - forward()では(x,y)の2引数を乗算\n",
    "  - backward()では\n",
    "      - 上流からの微分(dout)に対し順伝播の“ひっくり返した値”を乗算\n",
    "      - 下流に流す \n",
    " - リンゴ 2 個と消費税の計算グラフ（図5-16参照）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レイヤーを定義(順伝播、逆伝播)\n",
    "\n",
    "#--乗算レイヤ---------------------------\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):  #順伝播\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):  #逆伝播\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "\n",
    "#---加算レイヤ------------------------------\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total price: 220\n",
      "apple: 100\n",
      "apple_num: 2\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dTax: 200\n",
      "----------------\n",
      "dapple * apple = 220\n",
      "dapple_num * apple_num = 220\n",
      "dTax * tax = 220\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------\n",
    "# 乗算レイヤ(MulLayer)の例(2つのリンゴを購入,消費税1.1)\n",
    "#-------------------------------------------------\n",
    "from layer_naive import *\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"total price:\", int(price))\n",
    "print(\"apple:\", apple)\n",
    "print(\"apple_num:\", apple_num)\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dTax:\", dtax)\n",
    "print(\"----------------\")\n",
    "print(\"dapple * apple = \" + ('%0.0f' % (dapple * apple)) )\n",
    "print(\"dapple_num * apple_num = \" + ('%0.0f' % (dapple_num * apple_num)))\n",
    "print(\"dTax * tax = \" + ('%0.0f' % (dtax * tax)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dapple= 2.2\n",
      "dapple_num=110\n",
      "dtax= 200\n"
     ]
    }
   ],
   "source": [
    "# backward　\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print('dapple=',dapple)\n",
    "print('dapple_num=' + ('%0.0f'% dapple_num))\n",
    "print('dtax=',dtax) # 2.2\n",
    "\n",
    "# backward() の呼び出す順番は、forward() と逆の順番\n",
    "# backward() の引数は、「順伝播の際の出力変数に対する微分」を入力\n",
    "# たとえば、mul_apple_layer という乗算レイヤは、\n",
    "#   - 順伝播時にapple_price を出力\n",
    "#   - 逆伝播時にapple_price の微分値dapple_price を引数に設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.4.2 加算レイヤ(AddLayer)の実装\n",
    "\n",
    "```\n",
    "from layer_naive import *\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)\n",
    "\n",
    "\n",
    "#　加算レイヤでは初期化は必要ないので、__init__() では何も行わず\n",
    "#　(pass 記述は「何も行わない」という命令)\n",
    "# 加算レイヤの forward()では2 引数(x,y)を受け取り、加算して出力\n",
    "# backward() では上流から伝わってきた微分(dout)を、そのまま下流に流すだけ\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加算レイヤを乗算レイヤと組み合わせ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "#- リンゴ 2 個とみかん 3 個の買い物を実装(図5-17)\n",
    "#  - 消費税1.1\n",
    "#  - 加算レイヤと乗算レイヤ\n",
    "#  - ソースコードは ch05/buy_apple_orange.py\n",
    "from layer_naive import *\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()# 加算レイヤ加わった図5-15\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5 活性化関数レイヤの実装\n",
    "\n",
    "- 計算グラフの考え方をニューラルネットワークに適用\n",
    "- ニューラルネットワークを構成する「層(レイヤ)」を1クラスとして実装する\n",
    "- まずは、活性化関数である ReLU と Sigmoid レイヤ（143-146参照）を実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.1 ReLU レイヤ\n",
    "\n",
    "- 活性化関数 ReLU(Rectified Linear Unit)\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\qquad \\quad      y=x (x>0), 0 (x \\leq 0)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "上式の微分\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\qquad \\quad      \\partial y/\\partial x=1 (x>0), 0 (x \\leq 0)\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "- 順伝播時の入力である x が 0 より大きければ、逆伝播は\n",
    " 上流の値をそのまま下流に流します。\n",
    "- 逆に、順伝播時に x が 0 以下であれば、逆伝播では\n",
    "  下流への信号はそこでストップします。\n",
    "- 計算グラフで表すと図5-18\n",
    "\n",
    "-  ReLU レイヤの実装\n",
    " - ニューラルネットワークのレイヤの実装では、forward() や\n",
    "   backward() の引数には、NumPy の配列が入力されることを想定\n",
    " - ReLU レイヤの実装プログラムはcommon/layers.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()   #ランプ関数の実装\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout # 1\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu クラスは、インスタンス変数として mask という変数を\n",
    "持ちます。この mask変数は True/False からなるNumPy 配列で、\n",
    "順伝播の入力である x の要素で 0 以下の場所を True、\n",
    "それ以外(0 より大きい要素)を False として保持します。たとえば、\n",
    "次の例で示すように、True/False からなる NumPy 配列を mask\n",
    "変数は保持します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図5-18 で示すように、順伝播時の入力の値が 0 以下ならば、\n",
    "逆伝播の値は 0 になります。そのため、逆伝播では、順伝播時に\n",
    "保持した mask を使って、上流から伝播された dout に対して、\n",
    "mask の要素が True の場所を 0 に設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU レイヤは、回路における「スイッチ」のように機能します。順伝播時に電\n",
    "流が流れていればスイッチを ON にし、電流が流れなければスイッチを OFF\n",
    "にします。逆伝播時には、スイッチが ON であれば電流がそのまま流れ、OFF\n",
    "であればそれ以上電流は流れません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.2 Sigmoid レイヤ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#- シグモイド関数を実装 (式5.9) \n",
    "\n",
    "y =1/ (1 + exp(−x))\n",
    "\n",
    "- 式 (5.9) を計算グラフで表す(図5-19)\n",
    "- 逆伝播の流れ\n",
    "\n",
    "- ステップ 1\n",
    "「/」ノードは y =1x を表す。この微分は解析的に次式。∂y/∂x = −1/x^2= −y^2\n",
    "\n",
    "- ステップ 2\n",
    "「+」ノードは、上流の値を下流にそのまま流すだけ\n",
    "\n",
    "- ステップ 3\n",
    "「exp」ノードは y = exp(x) を表し、その微分は次の式で表されます。\n",
    "計算グラフでは、上流の値に対して、順伝播時の出力――\n",
    "この例では exp(−x)――を乗算して下流へ伝播。\n",
    "\n",
    "- ステップ 4\n",
    "「×」ノードは、順伝播時の値を“ひっくり返して”乗算。そのため、ここ\n",
    "では −1 を乗算\n",
    "\n",
    "- 逆伝播の出力は ∂L/∂y * y^2 * exp(−x) （図5-20 結果）\n",
    "- この値が下流にあるノードに伝播\n",
    "- ∂L/∂y y^2 exp(−x) という値が順伝播の入力 x\n",
    "- 図5-20 の計算グラフは、図5-21がグループ化した「sigmoid」ノード\n",
    "\n",
    "- 図5-20 の計算グラフと図5-21 の簡略版の計算グラフは、計算の結果は同じ\n",
    "- 簡略版の計算グラフのほうが、逆伝播の際の途中の計算を省略することができる\n",
    "- また、ノードをグループ化することによって、Sigmoid レイヤの中身は気にならず"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- ∂L/∂y y^2 exp(−x) は∂L/∂y y(1 − y)\n",
    "\n",
    "- 図5-21 Sigmoid レイヤの逆伝播は、順伝播の出力だけから計算可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- Sigmoid レイヤを Python で実装（図5-22）\n",
    "- common/layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out #偏微分した式(y*(1-y))\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 順伝播時=出力をインスタンス変数の out に保持\n",
    "- 逆伝播時=out 変数を使って計算を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 5.6 Affine / Softmax レイヤの実装\n",
    "### 5.6.1 Affine レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Y = np.dot(X, W) + B\n",
    "- 行列の積の計算は、要素数を一致させる必要有\n",
    "- X(2,)・W(2,3) = O (3,)　、括弧内は要素数\n",
    "- ニューラルネットワークの順伝播で行う行列の積は、幾何学の分野では「ア\n",
    "フィン変換」と呼ばれる。ここでは、アフィン変換を行う処理を\n",
    "「Affine レイヤ」という名前で実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- np.dot(X, W) + B の計算グラフ(図5-24)\n",
    "- これまで見てきた計算グラフは「スカラ値」がノード間を流れた\n",
    "- この例では「行列」がノード間を伝播\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 図5-24計算グラフの逆伝播について考える。\n",
    "- (∂L/∂X) =(∂L/∂Y)· W^T\n",
    "- (∂L/∂W) = X^T ·(∂L/∂Y)\n",
    "\n",
    "- T:転置\n",
    "- 図 5-25 Affineレイヤの逆伝播:変数が多次元配列。逆伝播の各変数下部に。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xと (∂L/∂X) 、Wと(∂L/∂W) は同じ形状(=要素数)に注意\n",
    "\n",
    "- X = (x0, x1, ··· , xn)\n",
    "- (∂L/∂X) =( ∂L/∂x0, ∂L/∂x1, ··· , ∂L/∂xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- なぜ行列の形状に注意するか\n",
    "　　- 行列の積では、対応する次元の要素数を一致させる必要あり\n",
    "\n",
    "- 図 5-26 = 行列の積(dotノード)の逆伝播\n",
    "    - 行列の対応次元の要素数を一致させるように積を組み立てることで導く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 バッチ版 Affine レイヤ\n",
    "\n",
    "- N 個のデータをまとめて順伝播する場合を考える\n",
    "- つまり、バッチ版の Affine レイヤ\n",
    "- (データのまとまりは「バッチ」と呼ぶ)\n",
    "\n",
    "- 図5-27=バッチ版のAffine レイヤ\n",
    "\n",
    "```\n",
    "先ほどの説明と異なる点は、入力である X の形状が (N, 2) になっただけ.\n",
    "逆伝播の際は、行列の形状に注意すれば、∂L/∂X と ∂L/∂W は前と\n",
    "同じように導出する。\n",
    "```\n",
    "\n",
    "- バイアスの加算に際しては、注意が必要\n",
    " - 順伝播の際のバイアスの加算は、X · W に対して、バイアスがそれぞれのデータに加算されます。\n",
    " - N=2(データが 2 個)とした場合、バイアスは、その 2 個のデータ\n",
    "  それぞれに対して(それぞれの計算結果に対して)加算される。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dot_W= [[ 0  0  0]\n",
      " [10 10 10]]\n",
      "X_dot_W+B= [[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------\n",
    "# バイアスの加算 (順伝播)\n",
    "#--------------------------------\n",
    "import numpy as np\n",
    "\n",
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "tmp = X_dot_W\n",
    "\n",
    "Y = X_dot_W+B\n",
    "\n",
    "print('X_dot_W=', tmp)\n",
    "print('X_dot_W+B=', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dY= [[1 2 3]\n",
      " [4 5 6]]\n",
      "dB= [5 7 9]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------\n",
    "# バイアスの加算 (逆伝播)\n",
    "#--------------------------------\n",
    "import numpy as np\n",
    "\n",
    "dY =  np.array([[1, 2, 3,], [4, 5, 6]])\n",
    "\n",
    "dB = np.sum(dY,axis=0)\n",
    "\n",
    "print('dY=', dY)\n",
    "print('dB=', dB)\n",
    "\n",
    "#この例では、データが 2 個(N=2)と仮定\n",
    "# バイアスの逆伝播は、2 個データに対しての微分を、データごとに合算\n",
    "# np.sum() で、0 番目の軸(データを単位とした軸)に対して (axis=0)の総和計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------Affine の実装-----------------\n",
    "#\n",
    "# common/layers.py \n",
    "# 入力データがテンソル(4 次元のデータ)の場合も考慮した実装\n",
    "#----------------------------------\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    def forward(self, x):\n",
    "        # tensor対応\n",
    "        self.original_x_shape=x.shape\n",
    "        x=x.shape ##### NOTE!!!\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss レイヤ\n",
    "\n",
    "- 最後に、出力層ソフトマックス関数\n",
    "  - Softmax レイヤは、入力された値を正規化(出力の和が\n",
    "  1になるように変形)して出力。\n",
    "  - 手書き数字認識（図5-28）\n",
    "  - 手書き数字認識は、10 クラス分類を行うため、Softmax \n",
    "  レイヤへの入力は 10 個ある。\n",
    "- ニューラルネットワークは、推論(inference)と学習の2フェーズ\n",
    "  - ニューラルネットワークの推論では、通常、Softmax レイヤは\n",
    "   使用しない。図5-28 のネットワークで推論を行う場合、\n",
    "   最後の Affine レイヤの出力を認識結果として用いる。\n",
    "  - ニューラルネットワークの正規化しない出力結果\n",
    "   (図5-28 Softmax前層の Affine レイヤ出力)は「スコア」と呼ぶ   \n",
    "  - ニューラルネットワークの推論で答えを1つだけ出す場合は、\n",
    "　スコア最大値だけに興味があるため、Softmaxレイヤは必要ない\n",
    "  - 一方、ニューラルネットワークの学習時には、Softmax レイヤが\n",
    "  必要になる\n",
    "  \n",
    "- 図 5-28 入力画像がAffine レイヤと ReLU レイヤによって変換され、\n",
    "  Softmax レイヤによって 10個の入力が正規化される。\n",
    "  - 「0」スコアは5.3、Softmax レイヤによって 0.008(0.8%)に変換される\n",
    "  - 「2」であるスコアは 10.1 であり 0.991(99.1%)に変換される  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 「Softmax-with-Loss レイヤ」実装\n",
    "  - 損失関数である交差エントロピー誤差(cross entropy error)も含める\n",
    "  - 図5-29=Softmax-with-Loss レイヤ(ソフトマックス関数と交差エントロピー誤差)の計算グラフ\n",
    "  - 付録A = Softmax-with-Loss レイヤの導出過程\n",
    "\n",
    "- 図 5-30= 「簡易版」Softmax-with-Loss レイヤの計算グラフ\n",
    "  - Softmax レイヤは、入力である (a1, a2, a3) を正規化\n",
    "  - (y1, y2, y3) を出力\n",
    "  - Cross Entropy Error レイヤは、Softmax の出力 (y1, y2, y3) と、\n",
    "   教師ラベルの (t1, t2, t3) を受け取り、損失 Lを出力\n",
    "   \n",
    "- 注目すべきは、図5-30逆伝播の結果\n",
    "  - Softmax レイヤからの逆伝播は(y1 − t1, y2 − t2, y3 − t3)“キレイ”な結果\n",
    "  - (y1, y2, y3) はSoftmax レイヤの出力、(t1, t2, t3) は教師データ\n",
    "  - (y1 − t1, y2 − t2, y3 − t3)は、Softmax レイヤの出力と教師ラベルの差分になる。\n",
    "  - ニューラルネットワークの逆伝播では、この差分誤差が前レイヤへ伝わる(学習の重要性質)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニューラルネットワークの学習の目的は、ニューラルネットワーク\n",
    "  の出力(Softmax の出力)を教師ラベルに近づけるように、重みパラメータを調整すること。\n",
    "- 先ほどの (y1 − t1, y2 − t2, y3 − t3) 結果は、まさに Softmax レイヤ\n",
    "  の出力と教師ラベルの差であり、現在のニューラルネットワークの出力と\n",
    "  教師ラベルの誤差を素直に表している\n",
    " \n",
    "  - 実は“キレイ”な結果になる様に、交差エントロピー誤差が設計された。\n",
    "  - 回帰問題では出力層に「恒等関数」を用い、損失関数として「2 乗和誤差」を用いる(「3.5 出力層の設計」参照)、\n",
    "　　これも同様の理由によります。つまり、「恒等関数」の損失関数として「2 乗和誤差」を用いると、逆伝播が\n",
    "  　(y1 − t1, y2 − t2, y3 − t3) という“キレイ”な結果になる\n",
    "\n",
    "- 具体例\n",
    "　　- 教師ラベル (0, 1, 0) 、Softmax レイヤの出力 (0.3, 0.2, 0.5) の場合\n",
    "      - 正解ラベルに対する確率は 0.2(20%)\n",
    "      - この時点のニューラルネットワークは正しい認識ができていない\n",
    "      - この場合、Softmax レイヤからの逆伝播は、(0.3, −0.8, 0.5) という大きな誤差を伝播する\n",
    "　　- 教師ラベル (0, 1, 0) 、Softmax レイヤの出力 (0.01, 0.99, 0) の場合\n",
    "  　　- このニューラルネットワークは、かなり正確に認識\n",
    "      - Softmax レイヤからの逆伝播は、(0.01, −0.01, 0) という小さな誤差に\n",
    "      - この小さな誤差が前レイヤに伝播していくが、誤差は小さいためSoftmax レイヤより前にあるレイヤが学習する内容も小さくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# Softmax-with-Loss \n",
    "#   - Softmaxと交差エントロピー誤差をまとめて実装\n",
    "#   - 逆伝播の値がy-tという非常にキレイな値となる\n",
    "#   - Softmaxには交差エントロピー誤差を、恒等関数の場合は二乗和誤差を損失関数として使う\n",
    "#--------------------------------------------------------------------------------------\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 損失\n",
    "        self.y = None # softmax の出力\n",
    "        self.t = None # 教師データ(one-hot vector)\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx\n",
    "\n",
    "# 逆伝播の際には、伝播する値をバッチの個数(batch_size)で割ることで、\n",
    "# データ 1 個あたりの誤差が前レイヤへ伝播する点に注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# 5.7 誤差逆伝播法の実装\n",
    "\n",
    "- 前節で実装したレイヤを組み合わせ、レゴブロックを作るようにニューラルネットワークを構築する\n",
    "    \n",
    "## 5.7.1 ニューラルネットワークの学習の全体図\n",
    "\n",
    "- 前提\n",
    "ニューラルネットワークは、適応可能な重みとバイアスがあり、この重みとバ\n",
    "イアスを訓練データに適応するように調整することを「学習」と呼ぶ。ニュー\n",
    "ラルネットワークの学習は次の 4手順で行う。\n",
    "\n",
    "  - ステップ 1(ミニバッチ)\n",
    "    訓練データ中からランダムに一部データを選出\n",
    "\n",
    "  - ステップ 2(勾配の算出)\n",
    "    各重みパラメータに関する損失関数の勾配を計算\n",
    "\n",
    "  - ステップ 3(パラメータの更新) \n",
    "   重みパラメータを勾配方向に微小量だけ更新\n",
    "\n",
    "  - ステップ 4(繰り返す)\n",
    "\n",
    "  - ステップ 1、ステップ 2、ステップ 3 を繰り返す\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装\n",
    "\n",
    "- 2 層のニューラルネットワークをTwoLayerNet として実装\n",
    "- クラスのインスタンス変数とメソッドを整理 (表5-1 と表5-2 )\n",
    "\n",
    "- レイヤを使用することによって、認識結果を得る処理(predict())\n",
    "や勾配を求める処理(gradient())がレイヤの伝播だけで達成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append('./')  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from saitobook.common.layers import *\n",
    "from saitobook.common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成（単純計算のまとまり）\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        #--tが必要なので上と分けた\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    #------------------------------------------------\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward----------------------------------\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        #--------------------------------------------    \n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特にニューラルネットワークのレイヤを OrderedDict として保持する点が重要\n",
    "\n",
    "- OrderedDict=順番付きのディクショナリ\n",
    "  - 「順番付き」とは、ディクショナリに追加した要素の順番を覚える\n",
    "  - ニューラルネットワークの順伝播では、追加した順にレイヤの forward() メソッドを呼び出すだけで処理が完了\n",
    "- 逆伝播では、逆の順番でレイヤを呼び出すだけ\n",
    "  - Affine レイヤやReLU レイヤが、内部で順伝播と逆伝播を正しく\n",
    "   処理してくれる\n",
    "  - ここで行うことはレイヤを正しい順番で連結し、順番に(or逆順に)レイヤを呼び出すだけ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.7.3 誤差逆伝播法の勾配確認\n",
    "\n",
    "\n",
    "- これまで勾配を求める方法を2つ説明してきた。\n",
    "  - 1) 数値微分によって求める方法\n",
    "  - 2) 解析的に数式を解いて求める方法\n",
    "\n",
    "- これからは誤差逆伝播法で勾配を求めることにしましょう。\n",
    "  - 後者の解析的に求める方法について言えば、誤差逆伝播法を用いることで、大量のパラメータが存在しても効率的に計算できた\n",
    "  - 計算に時間のかかる数値微分ではなく、誤差逆伝播法で勾配を求めることにしましょう\n",
    "\n",
    "- 数値微分の利点は、実装が簡単であるということ\n",
    "  - 勾配確認(gradient check)= 数値微分で勾配を求めた結果と、誤差逆伝播法で求めた勾配の結果が一致すること\n",
    "    を確認する作業\n",
    "  -(ソースコードは ch05/gradient_check.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# 実行時間差を計算する(多田さんscriptより)\n",
    "#-------------------------------------------------\n",
    "import time\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time.time()\n",
    "        self.lap_start = self.start\n",
    "        self.lap_list = []\n",
    "\n",
    "    def lap_show (self):\n",
    "        now = time.time()\n",
    "        lap = now - self.lap_start\n",
    "        self.lap_list.append(lap)\n",
    "        self.lap_start = now\n",
    "        print(\"lap \" + str(len(self.lap_list)) + ((\": %0.7f sec\") %lap) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実行速度 lap1: 数値微分、lap2: 誤差逆伝播法\n",
      "lap 1: 6.8508735 sec\n",
      "lap 2: 0.0000000 sec\n",
      "W1:2.0390068474371752e-10\n",
      "b1:8.501201741802251e-10\n",
      "W2:6.955964897092937e-08\n",
      "b2:1.3883158105770788e-07\n",
      "数値微分time=6.850873e+00\n",
      "誤差逆伝播time=0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------\n",
    "#  誤差逆伝播法(original)\n",
    "#-------------------------------------\n",
    "import sys, os\n",
    "sys.path.append('./')  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from saitobook.dataset.mnist import load_mnist\n",
    "from saitobook.samples.two_layer_net_mod import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "#---------------------------------------------\n",
    "timer = Timer()\n",
    "print(\"実行速度 lap1: 数値微分、lap2: 誤差逆伝播法\")\n",
    "#--------------------------------------------\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "timer.lap_show()\n",
    "\n",
    "#--------------------------------------------\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "timer.lap_show()\n",
    "#--------------------------------------------\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n",
    "    \n",
    "#--------------------------------------------    \n",
    "print('数値微分time=%e' % timer.lap_list[0])\n",
    "print('誤差逆伝播time=%e' % timer.lap_list[1])\n",
    "if timer.lap_list[1] > 0:\n",
    "    print((\"数値微分/誤差逆伝播法=%0.1f倍\" % (timer.lap_list[0] / timer.lap_list[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
